version: '3.8'

services:
  # Backend service (FastAPI)
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: langchain-chat-backend
    volumes:
      - ./backend:/app/backend
      - ./backend/notebooks:/app/backend/notebooks
    environment:
      - PYTHONPATH=/app
      - OLLAMA_BASE_URL=http://llm:11434  # For LLM service communication
      - JUPYTER_TOKEN=dev  # Default password for Jupyter
    ports:
      - "8000:8000"
      - "8888:8888"  # Jupyter port
    command: >
      sh -c "jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=$$JUPYTER_TOKEN --notebook-dir=/app/backend/notebooks &
             cd /app/backend && python -c 'import time; import requests; print(\"Waiting for Ollama to be ready...\"); [time.sleep(5) for _ in range(12) if not requests.get(\"http://llm:11434/api/tags\", timeout=5).ok]' && \
             uvicorn api:app --host 0.0.0.0 --port 8000 --reload"
    depends_on:
      llm:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Frontend service (Vite + React)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: langchain-chat-frontend
    ports:
      - "3000:80"
    environment:
      - VITE_API_URL=http://localhost:8000  # Update this to your backend URL if different
    depends_on:
      - backend
    restart: unless-stopped

  # LLM service (Ollama)
  llm:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: langchain-chat-llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

volumes:
  ollama_data:
